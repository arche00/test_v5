#!/usr/bin/env python3
"""
ì˜ˆì¸¡ê°’ ì €ì¥ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Streamlit ì—†ì´ ì§ì ‘ ì‹¤í–‰
"""
import sqlite3
import pandas as pd
import os
import sys
from collections import Counter, defaultdict

# DB ê²½ë¡œ
DB_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'hypothesis_validation.db')

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    try:
        if not os.path.exists(DB_PATH):
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DB_PATH}")
            return None
        return sqlite3.connect(DB_PATH)
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return None

def load_ngram_chunks(window_size, grid_string_ids=None):
    """ngram_chunks ë¡œë“œ"""
    conn = get_db_connection()
    if conn is None:
        return pd.DataFrame()
    
    try:
        if grid_string_ids is None:
            query = """
                SELECT grid_string_id, prefix, suffix
                FROM ngram_chunks
                WHERE window_size = ?
            """
            params = [window_size]
        else:
            placeholders = ','.join(['?'] * len(grid_string_ids))
            query = f"""
                SELECT grid_string_id, prefix, suffix
                FROM ngram_chunks
                WHERE window_size = ? AND grid_string_id IN ({placeholders})
            """
            params = [window_size] + grid_string_ids
        
        df = pd.read_sql_query(query, conn, params=params)
        return df
    except Exception as e:
        print(f"âŒ ngram_chunks ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()
    finally:
        conn.close()

def build_frequency_model(ngrams_df):
    """ë¹ˆë„ ê¸°ë°˜ ëª¨ë¸ êµ¬ì¶•"""
    model = defaultdict(lambda: Counter())
    for _, row in ngrams_df.iterrows():
        prefix = row['prefix']
        suffix = row['suffix']
        model[prefix][suffix] += 1
    return dict(model)

def predict_frequency(model, prefix):
    """ë¹ˆë„ ê¸°ë°˜ ì˜ˆì¸¡"""
    if prefix not in model:
        return None, {}
    
    counter = model[prefix]
    total = sum(counter.values())
    if total == 0:
        return None, {}
    
    # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ suffix ì„ íƒ
    predicted = counter.most_common(1)[0][0]
    
    # ë¹„ìœ¨ ê³„ì‚°
    ratios = {suffix: (count / total * 100) for suffix, count in counter.items()}
    
    return predicted, ratios

def predict_for_prefix(model, prefix, method="ë¹ˆë„ ê¸°ë°˜"):
    """ë‹¨ì¼ prefixì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰"""
    if method == "ë¹ˆë„ ê¸°ë°˜":
        predicted, ratios = predict_frequency(model, prefix)
    else:
        predicted, ratios = predict_frequency(model, prefix)  # ê¸°ë³¸ê°’
    
    confidence = max(ratios.values()) if ratios else 0.0
    
    return {
        'predicted': predicted,
        'ratios': ratios,
        'confidence': confidence
    }

def predict_confidence_threshold(model, prefix, method="ë¹ˆë„ ê¸°ë°˜", threshold=60):
    """ì‹ ë¢°ë„ ì„ê³„ê°’ ì „ëµ"""
    result = predict_for_prefix(model, prefix, method)
    confidence = result.get('confidence', 0.0)
    predicted = result.get('predicted')
    
    if predicted is None:
        return {
            'predicted': None,
            'ratios': result.get('ratios', {}),
            'confidence': confidence,
            'strategy_name': f'ì‹ ë¢°ë„ì„ê³„ê°’_{threshold}'
        }
    
    confidence_rounded = round(confidence, 1)
    threshold_rounded = round(threshold, 1)
    
    if confidence_rounded < threshold_rounded:
        return {
            'predicted': None,
            'ratios': result.get('ratios', {}),
            'confidence': confidence,
            'strategy_name': f'ì‹ ë¢°ë„ì„ê³„ê°’_{threshold}'
        }
    
    return {
        'predicted': result.get('predicted'),
        'ratios': result.get('ratios', {}),
        'confidence': confidence,
        'strategy_name': f'ì‹ ë¢°ë„ì„ê³„ê°’_{threshold}'
    }

def save_predictions(cutoff_grid_string_id=None, window_sizes=[5, 6, 7, 8, 9],
                     methods=["ë¹ˆë„ ê¸°ë°˜"], thresholds=[0, 50, 60, 70, 80, 90, 100],
                     batch_size=1000):
    """ì˜ˆì¸¡ê°’ ì €ì¥"""
    conn = get_db_connection()
    if conn is None:
        return None
    
    try:
        # ì´ì „ ë°ì´í„° ì„ íƒ
        if cutoff_grid_string_id is None:
            query = "SELECT id FROM preprocessed_grid_strings ORDER BY id"
            params = []
        else:
            query = "SELECT id FROM preprocessed_grid_strings WHERE id <= ? ORDER BY id"
            params = [cutoff_grid_string_id]
        
        df_historical = pd.read_sql_query(query, conn, params=params)
        
        if len(df_historical) == 0:
            print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'total_saved': 0,
                'new_records': 0,
                'updated_records': 0,
                'unique_prefixes': 0
            }
        
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(df_historical)}ê°œ")
        
        historical_ids = df_historical['id'].tolist()
        
        total_saved = 0
        new_records = 0
        updated_records = 0
        unique_prefixes_set = set()
        
        cursor = conn.cursor()
        
        for window_size in window_sizes:
            print(f"\nğŸ”„ window_size={window_size} ì²˜ë¦¬ ì¤‘...")
            train_ngrams = load_ngram_chunks(window_size=window_size, grid_string_ids=historical_ids)
            
            if len(train_ngrams) == 0:
                print(f"  âš ï¸  ngram_chunksê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            print(f"  ğŸ“Š ngram_chunks: {len(train_ngrams):,}ê°œ")
            
            # ëª¨ë¸ êµ¬ì¶•
            for method in methods:
                print(f"  ğŸ”¨ ëª¨ë¸ êµ¬ì¶• ì¤‘ (method={method})...")
                if method == "ë¹ˆë„ ê¸°ë°˜":
                    model = build_frequency_model(train_ngrams)
                else:
                    model = build_frequency_model(train_ngrams)
                
                # ëª¨ë“  ê°€ëŠ¥í•œ prefix ì¶”ì¶œ
                all_prefixes = set(train_ngrams['prefix'].unique())
                print(f"  ğŸ“‹ ê³ ìœ  prefix: {len(all_prefixes):,}ê°œ")
                
                # ê° prefixì— ëŒ€í•´ ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì €ì¥
                batch_data = []
                processed = 0
                
                for prefix in all_prefixes:
                    unique_prefixes_set.add((window_size, prefix))
                    
                    # ê° ì„ê³„ê°’ì— ëŒ€í•´ ì˜ˆì¸¡ê°’ ê³„ì‚°
                    for threshold in thresholds:
                        if threshold == 0:
                            prediction_result = predict_for_prefix(model, prefix, method)
                        else:
                            prediction_result = predict_confidence_threshold(model, prefix, method, threshold)
                        
                        predicted = prediction_result.get('predicted')
                        confidence = prediction_result.get('confidence', 0.0)
                        ratios = prediction_result.get('ratios', {})
                        
                        b_ratio = ratios.get('b', 0.0)
                        p_ratio = ratios.get('p', 0.0)
                        
                        batch_data.append((
                            window_size,
                            prefix,
                            predicted,
                            confidence,
                            b_ratio,
                            p_ratio,
                            method,
                            threshold
                        ))
                    
                    processed += 1
                    if processed % 1000 == 0:
                        print(f"    ì§„í–‰: {processed:,}/{len(all_prefixes):,} prefix ì²˜ë¦¬ë¨")
                
                # ë°°ì¹˜ë¡œ ì €ì¥/ì—…ë°ì´íŠ¸
                if batch_data:
                    print(f"  ğŸ’¾ {len(batch_data):,}ê°œ ì˜ˆì¸¡ê°’ ì €ì¥ ì¤‘...")
                    for i in range(0, len(batch_data), batch_size):
                        batch = batch_data[i:i + batch_size]
                        
                        for item in batch:
                            try:
                                # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
                                cursor.execute('''
                                    SELECT id FROM stored_predictions
                                    WHERE window_size = ? 
                                      AND prefix = ? 
                                      AND method = ? 
                                      AND threshold = ?
                                ''', (item[0], item[1], item[6], item[7]))
                                
                                existing = cursor.fetchone()
                                
                                cursor.execute('''
                                    INSERT OR REPLACE INTO stored_predictions (
                                        window_size, prefix,
                                        predicted_value, confidence, b_ratio, p_ratio,
                                        method, threshold, updated_at
                                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now', '+9 hours'))
                                ''', item)
                                
                                if existing:
                                    updated_records += 1
                                else:
                                    new_records += 1
                                
                                total_saved += 1
                            except Exception as e:
                                print(f"    âš ï¸  ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                                continue
                
                print(f"  âœ… window_size={window_size}, method={method} ì™„ë£Œ")
        
        conn.commit()
        
        return {
            'total_saved': total_saved,
            'new_records': new_records,
            'updated_records': updated_records,
            'unique_prefixes': len(unique_prefixes_set)
        }
        
    except Exception as e:
        conn.rollback()
        print(f"âŒ ì˜ˆì¸¡ê°’ ì €ì¥/ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        conn.close()

if __name__ == "__main__":
    print("=" * 60)
    print("ì˜ˆì¸¡ê°’ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("=" * 60)
    
    # ìƒíƒœ í™•ì¸
    conn = get_db_connection()
    if conn:
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM stored_predictions")
        current_count = cursor.fetchone()[0]
        conn.close()
        
        print(f"\ní˜„ì¬ stored_predictions ë ˆì½”ë“œ ìˆ˜: {current_count:,}ê°œ")
        
        if current_count == 0:
            print("\nì˜ˆì¸¡ê°’ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("(ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n")
            
            result = save_predictions(
                cutoff_grid_string_id=None,  # ì „ì²´ ë°ì´í„° ì‚¬ìš©
                window_sizes=[5, 6, 7, 8, 9],
                methods=["ë¹ˆë„ ê¸°ë°˜"],
                thresholds=[0, 50, 60, 70, 80, 90, 100],
                batch_size=1000
            )
            
            if result:
                print("\n" + "=" * 60)
                print("âœ… ì˜ˆì¸¡ê°’ ì €ì¥ ì™„ë£Œ!")
                print("=" * 60)
                print(f"ì´ ì €ì¥/ì—…ë°ì´íŠ¸: {result['total_saved']:,}ê°œ")
                print(f"ìƒˆ ë ˆì½”ë“œ: {result['new_records']:,}ê°œ")
                print(f"ì—…ë°ì´íŠ¸: {result['updated_records']:,}ê°œ")
                print(f"ê³ ìœ  Prefix ìˆ˜: {result['unique_prefixes']:,}ê°œ")
                
                # ì €ì¥ í›„ í™•ì¸
                conn = get_db_connection()
                if conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM stored_predictions")
                    new_count = cursor.fetchone()[0]
                    conn.close()
                    print(f"\nì €ì¥ í›„ ë ˆì½”ë“œ ìˆ˜: {new_count:,}ê°œ")
            else:
                print("\nâŒ ì˜ˆì¸¡ê°’ ì €ì¥ ì‹¤íŒ¨")
        else:
            print(f"\nâœ… ì´ë¯¸ {current_count:,}ê°œì˜ ì˜ˆì¸¡ê°’ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("ë‹¤ì‹œ ì €ì¥í•˜ë ¤ë©´ stored_predictions í…Œì´ë¸”ì„ ë¹„ìš°ê³  ì‹¤í–‰í•˜ì„¸ìš”.")


