"""
Prefix Suffix ì˜ˆì¸¡ ë¹„êµ ë° ê²€ì¦ ì•±
Prefixë³„ë¡œ ì—¬ëŸ¬ ì˜ˆì¸¡ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , íŒ¨í„´ ê²€ì¶œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©°,
ì‹œê³„ì—´ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ ì¸í„°ë™í‹°ë¸Œ ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” ë…ë¦½ì ì¸ ì•±
"""

import streamlit as st
import sqlite3
import pandas as pd
import numpy as np
import os
from collections import Counter, defaultdict
from datetime import datetime
from math import log2
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Prefix Suffix Prediction Comparison",
    page_icon="ğŸ“Š",
    layout="wide"
)

# DB ê²½ë¡œ ì„¤ì •
DB_PATH = 'hypothesis_validation.db'

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ë³µì œ)
# ============================================================================

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    try:
        db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), DB_PATH)
        if not os.path.exists(db_path):
            st.error(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
            return None
        return sqlite3.connect(db_path)
    except Exception as e:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return None

def load_preprocessed_data():
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
    try:
        conn = get_db_connection()
        if conn is None:
            return pd.DataFrame()
        
        query = """
            SELECT 
                id,
                source_session_id,
                source_id,
                grid_string,
                string_length,
                b_count,
                p_count,
                b_ratio,
                p_ratio,
                created_at,
                processed_at
            FROM preprocessed_grid_strings
            ORDER BY created_at ASC
        """
        df = pd.read_sql_query(query, conn)
        conn.close()
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()

def load_ngram_chunks(window_size=None, grid_string_ids=None):
    """N-gram ì¡°ê° ë¡œë“œ"""
    conn = None
    try:
        conn = get_db_connection()
        if conn is None:
            return pd.DataFrame()
        
        query = """
            SELECT 
                id,
                grid_string_id,
                window_size,
                chunk_index,
                prefix,
                suffix,
                full_chunk
            FROM ngram_chunks
            WHERE 1=1
        """
        params = []
        
        if window_size is not None:
            query += " AND window_size = ?"
            params.append(window_size)
        
        if grid_string_ids is not None and len(grid_string_ids) > 0:
            # SQLiteì˜ íŒŒë¼ë¯¸í„° ì œí•œ(999ê°œ)ì„ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            if len(grid_string_ids) > 900:
                # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                all_dfs = []
                batch_size = 900
                for i in range(0, len(grid_string_ids), batch_size):
                    batch_ids = grid_string_ids[i:i + batch_size]
                    placeholders = ','.join(['?'] * len(batch_ids))
                    batch_query = query + f" AND grid_string_id IN ({placeholders})"
                    batch_query += " ORDER BY grid_string_id, window_size, chunk_index"
                    batch_df = pd.read_sql_query(batch_query, conn, params=params + batch_ids)
                    all_dfs.append(batch_df)
                
                if all_dfs:
                    df = pd.concat(all_dfs, ignore_index=True)
                else:
                    df = pd.DataFrame()
            else:
                placeholders = ','.join(['?'] * len(grid_string_ids))
                query += f" AND grid_string_id IN ({placeholders})"
                params.extend(grid_string_ids)
                query += " ORDER BY grid_string_id, window_size, chunk_index"
                df = pd.read_sql_query(query, conn, params=params)
        else:
            query += " ORDER BY grid_string_id, window_size, chunk_index"
            df = pd.read_sql_query(query, conn, params=params)
        
        return df
    except Exception as e:
        st.error(f"N-gram ì¡°ê° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()
    finally:
        if conn:
            conn.close()

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ë³µì œ)
# ============================================================================

def calculate_prefix_ratio(prefix):
    """
    prefix ë¬¸ìì—´ì˜ b/p ë¹„ìœ¨ ê³„ì‚°
    
    Args:
        prefix: prefix ë¬¸ìì—´ (ì˜ˆ: "bbbbpp")
    
    Returns:
        dict: {'b_ratio': float, 'p_ratio': float, 'b_count': int, 'p_count': int}
    """
    b_count = prefix.count('b')
    p_count = prefix.count('p')
    total = len(prefix)
    
    if total == 0:
        return {'b_ratio': 0.5, 'p_ratio': 0.5, 'b_count': 0, 'p_count': 0}
    
    return {
        'b_ratio': b_count / total,
        'p_ratio': p_count / total,
        'b_count': b_count,
        'p_count': p_count
    }

def extract_prefixes_from_string(grid_string, window_size):
    """
    grid_stringì„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ prefixì™€ suffix ì¶”ì¶œ
    
    Args:
        grid_string: ì…ë ¥ ë¬¸ìì—´ (ì˜ˆ: "bbbbppbbpp...")
        window_size: ìœˆë„ìš° í¬ê¸°
    
    Returns:
        list: [(prefix, suffix, index), ...]
    """
    if not grid_string or len(grid_string) < window_size:
        return []
    
    prefixes = []
    for i in range(len(grid_string) - window_size + 1):
        chunk = grid_string[i:i + window_size]
        prefix = chunk[:-1]  # ì• N-1ê°œ
        suffix = chunk[-1]   # ë§ˆì§€ë§‰ 1ê°œ
        prefixes.append((prefix, suffix, i))
    
    return prefixes

def get_next_prefix(current_prefix, value, window_size):
    """
    í˜„ì¬ prefixì™€ ì„ íƒëœ ê°’ìœ¼ë¡œ ë‹¤ìŒ prefix ìƒì„±
    
    Args:
        current_prefix: í˜„ì¬ prefix
        value: ì„ íƒëœ ê°’ ('b' ë˜ëŠ” 'p')
        window_size: ìœˆë„ìš° í¬ê¸°
    
    Returns:
        str: ë‹¤ìŒ prefix (ë§ˆì§€ë§‰ N-1ìë¦¬)
    """
    new_string = current_prefix + value
    # ìœˆë„ìš° í¬ê¸°ê°€ Nì´ë©´ prefixëŠ” N-1ìë¦¬
    prefix_length = window_size - 1
    if len(new_string) >= prefix_length:
        return new_string[-prefix_length:]
    return new_string

def generate_and_save_prefix_trend_rules(window_size, grid_string_ids=None):
    """
    prefix ë¹„ìœ¨ ê·œì¹™ ìƒì„± ë° DB ì €ì¥
    
    ngram_chunks í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ê° prefixë³„ë¡œ 
    b/p ë¹„ìœ¨ê³¼ suffix ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œ ê·œì¹™ì„ ìƒì„±í•˜ê³  ì €ì¥
    
    Args:
        window_size: ìœˆë„ìš° í¬ê¸°
        grid_string_ids: íŠ¹ì • grid_string_id ëª©ë¡ (Noneì´ë©´ ì „ì²´)
    
    Returns:
        int: ì €ì¥ëœ ê·œì¹™ ìˆ˜
    """
    conn = get_db_connection()
    if conn is None:
        return 0
    
    try:
        # ngram_chunksì—ì„œ ë°ì´í„° ë¡œë“œ
        if grid_string_ids is not None and len(grid_string_ids) > 0:
            if len(grid_string_ids) > 900:
                # ë°°ì¹˜ë¡œ ì²˜ë¦¬
                all_ngrams = []
                batch_size = 900
                for i in range(0, len(grid_string_ids), batch_size):
                    batch_ids = grid_string_ids[i:i + batch_size]
                    placeholders = ','.join(['?'] * len(batch_ids))
                    query = f"""
                        SELECT prefix, suffix
                        FROM ngram_chunks
                        WHERE window_size = ? AND grid_string_id IN ({placeholders})
                    """
                    batch_df = pd.read_sql_query(query, conn, params=[window_size] + batch_ids)
                    all_ngrams.append(batch_df)
                
                if all_ngrams:
                    ngrams_df = pd.concat(all_ngrams, ignore_index=True)
                else:
                    ngrams_df = pd.DataFrame()
            else:
                placeholders = ','.join(['?'] * len(grid_string_ids))
                query = f"""
                    SELECT prefix, suffix
                    FROM ngram_chunks
                    WHERE window_size = ? AND grid_string_id IN ({placeholders})
                """
                ngrams_df = pd.read_sql_query(query, conn, params=[window_size] + grid_string_ids)
        else:
            query = """
                SELECT prefix, suffix
                FROM ngram_chunks
                WHERE window_size = ?
            """
            ngrams_df = pd.read_sql_query(query, conn, params=[window_size])
        
        if len(ngrams_df) == 0:
            return 0
        
        # prefixë³„ë¡œ suffix ë¶„í¬ ë¶„ì„
        prefix_analysis = defaultdict(lambda: {'b': 0, 'p': 0})
        
        for _, row in ngrams_df.iterrows():
            prefix = row['prefix']
            suffix = row['suffix']
            prefix_analysis[prefix][suffix] += 1
        
        # ê·œì¹™ ê³„ì‚° ë° ì €ì¥
        cursor = conn.cursor()
        saved_count = 0
        
        for prefix, suffix_counts in prefix_analysis.items():
            # prefixì˜ b/p ë¹„ìœ¨ ê³„ì‚°
            prefix_ratio = calculate_prefix_ratio(prefix)
            b_ratio = prefix_ratio['b_ratio']
            
            # suffix ë¶„í¬
            b_suffix_count = suffix_counts['b']
            p_suffix_count = suffix_counts['p']
            total_count = b_suffix_count + p_suffix_count
            
            if total_count == 0:
                continue
            
            b_suffix_ratio = b_suffix_count / total_count
            p_suffix_ratio = p_suffix_count / total_count
            
            # ê·œì¹™ ê²°ì •: íŠ¸ë Œë“œ ë”°ë¦„ vs ë°˜ëŒ€
            if b_ratio > 0.5:  # prefixì—ì„œ bê°€ ë§ìŒ
                # suffixë„ bê°€ ë§ìœ¼ë©´ íŠ¸ë Œë“œ ë”°ë¦„
                trend_follow = 1 if b_suffix_ratio > p_suffix_ratio else 0
            elif b_ratio < 0.5:  # prefixì—ì„œ pê°€ ë§ìŒ
                # suffixë„ pê°€ ë§ìœ¼ë©´ íŠ¸ë Œë“œ ë”°ë¦„
                trend_follow = 1 if p_suffix_ratio > b_suffix_ratio else 0
            else:  # ê· í˜• (50%)
                # ì°¨ì´ê°€ ì‘ìœ¼ë©´ íŠ¸ë Œë“œ ë”°ë¦„ìœ¼ë¡œ ê°„ì£¼
                trend_follow = 1 if abs(b_suffix_ratio - p_suffix_ratio) < 0.2 else 0
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = abs(b_suffix_ratio - p_suffix_ratio)
            
            # DBì— ì €ì¥ (INSERT OR REPLACE)
            cursor.execute('''
                INSERT OR REPLACE INTO prefix_trend_rules (
                    window_size, prefix, b_ratio, p_ratio,
                    b_suffix_count, p_suffix_count, total_count,
                    trend_follow, confidence, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now', '+9 hours'))
            ''', (
                window_size, prefix, b_ratio, prefix_ratio['p_ratio'],
                b_suffix_count, p_suffix_count, total_count,
                trend_follow, confidence
            ))
            
            if cursor.rowcount > 0:
                saved_count += 1
        
        conn.commit()
        return saved_count
        
    except Exception as e:
        conn.rollback()
        st.error(f"prefix_trend_rules ìƒì„± ë° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return 0
    finally:
        conn.close()

def load_prefix_trend_rules(window_size):
    """
    DBì—ì„œ prefix ë¹„ìœ¨ ê·œì¹™ ë¡œë“œ
    
    Args:
        window_size: ìœˆë„ìš° í¬ê¸°
    
    Returns:
        dict: {prefix: {'b_ratio': float, 'p_ratio': float, 'trend_follow': bool, 'confidence': float, ...}, ...}
    """
    conn = get_db_connection()
    if conn is None:
        return {}
    
    try:
        query = """
            SELECT prefix, b_ratio, p_ratio, b_suffix_count, p_suffix_count,
                   total_count, trend_follow, confidence
            FROM prefix_trend_rules
            WHERE window_size = ?
        """
        df = pd.read_sql_query(query, conn, params=[window_size])
        
        if len(df) == 0:
            return {}
        
        rules = {}
        for _, row in df.iterrows():
            rules[row['prefix']] = {
                'b_ratio': row['b_ratio'],
                'p_ratio': row['p_ratio'],
                'b_suffix_count': int(row['b_suffix_count']),
                'p_suffix_count': int(row['p_suffix_count']),
                'total_count': int(row['total_count']),
                'trend_follow': bool(row['trend_follow']),
                'confidence': row['confidence']
            }
        
        return rules
        
    except Exception as e:
        st.error(f"prefix_trend_rules ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return {}
    finally:
        conn.close()

# ============================================================================
# ëª¨ë¸ êµ¬ì¶• í•¨ìˆ˜ë“¤ (ë³µì œ)
# ============================================================================

def build_frequency_model(ngrams_df):
    """
    ë¹ˆë„ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
    
    Args:
        ngrams_df: N-gram ì¡°ê° DataFrame
    
    Returns:
        dict: {prefix: {suffix: count, ...}, ...}
    """
    model = defaultdict(lambda: Counter())
    
    for _, row in ngrams_df.iterrows():
        prefix = row['prefix']
        suffix = row['suffix']
        model[prefix][suffix] += 1
    
    return dict(model)

def build_weighted_model(ngrams_df, weight_decay=0.95):
    """
    ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëª¨ë¸ êµ¬ì¶•
    ìµœê·¼ ì¡°ê°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    
    Args:
        ngrams_df: N-gram ì¡°ê° DataFrame
        weight_decay: ê°€ì¤‘ì¹˜ ê°ì‡ ìœ¨ (0~1)
    
    Returns:
        dict: {prefix: {suffix: weighted_count, ...}, ...}
    """
    model = defaultdict(lambda: defaultdict(float))
    
    # grid_string_idë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ìˆœì„œ ë³´ì¡´
    grouped = ngrams_df.groupby('grid_string_id')
    
    for grid_string_id, group_df in grouped:
        # ìµœê·¼ ì¡°ê°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        group_df = group_df.sort_values('chunk_index')
        max_index = len(group_df)
        
        for idx, (_, row) in enumerate(group_df.iterrows()):
            # ê°€ì¤‘ì¹˜: ìµœê·¼ ì¡°ê°ì¼ìˆ˜ë¡ ë†’ìŒ
            weight = weight_decay ** (max_index - idx - 1)
            
            prefix = row['prefix']
            suffix = row['suffix']
            model[prefix][suffix] += weight
    
    return dict(model)

def build_safety_first_model(ngrams_df):
    """
    ì•ˆì „ ìš°ì„  ì ì‘í˜• ëª¨ë¸ êµ¬ì¶• (ë…ë¦½ì  êµ¬í˜„)
    
    ë¹ˆë„ ê¸°ë°˜ ëª¨ë¸ì„ ë‚´ë¶€ì—ì„œ ì§ì ‘ êµ¬ì¶•í•˜ì—¬ ê¸°ì¡´ í•¨ìˆ˜ì™€ ë…ë¦½ì 
    
    Args:
        ngrams_df: N-gram ì¡°ê° DataFrame
    
    Returns:
        dict: {prefix: {suffix: count, ...}, ...}
    """
    model = defaultdict(lambda: Counter())
    
    for _, row in ngrams_df.iterrows():
        prefix = row['prefix']
        suffix = row['suffix']
        model[prefix][suffix] += 1
    
    return dict(model)

def build_balance_recovery_trend_model_final(ngrams_df, window_size):
    """
    ê· í˜• íšŒë³µ íŠ¸ë Œë“œ ëª¨ë¸ êµ¬ì¶• (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: DB ìš°ì„ , ì—†ìœ¼ë©´ ê³„ì‚° í›„ ì €ì¥)
    
    Args:
        ngrams_df: N-gram ì¡°ê° DataFrame
        window_size: ìœˆë„ìš° í¬ê¸°
    
    Returns:
        dict: {'prefix_rules': dict, 'frequency_model': dict}
    """
    # 1. DBì—ì„œ ê·œì¹™ ë¡œë“œ ì‹œë„
    prefix_rules = load_prefix_trend_rules(window_size)
    
    # 2. ê·œì¹™ì´ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ê³„ì‚° í›„ ì €ì¥
    if not prefix_rules:
        # ngrams_dfì—ì„œ ê·œì¹™ ê³„ì‚°
        saved_count = generate_and_save_prefix_trend_rules(window_size)
        if saved_count > 0:
            # ë‹¤ì‹œ ë¡œë“œ
            prefix_rules = load_prefix_trend_rules(window_size)
    
    # 3. ë¹ˆë„ ëª¨ë¸ êµ¬ì¶• (í•­ìƒ ì˜ˆì¸¡ê°’ ë°˜í™˜ ë³´ì¥)
    frequency_model = build_frequency_model(ngrams_df)
    
    return {
        'prefix_rules': prefix_rules,
        'frequency_model': frequency_model
    }

# ============================================================================
# ì˜ˆì¸¡ í•¨ìˆ˜ë“¤ (ë³µì œ)
# ============================================================================

def predict_frequency(model, prefix):
    """ë¹ˆë„ ê¸°ë°˜ ì˜ˆì¸¡"""
    if prefix not in model:
        return None, {}
    
    suffix_counts = model[prefix]
    if not suffix_counts:
        return None, {}
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ suffix
    most_common = suffix_counts.most_common(1)[0]
    predicted = most_common[0]
    
    # ë¹„ìœ¨ ê³„ì‚°
    total = sum(suffix_counts.values())
    ratios = {suffix: (count / total * 100) for suffix, count in suffix_counts.items()}
    
    return predicted, ratios

def predict_weighted(model, prefix):
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì˜ˆì¸¡"""
    if prefix not in model:
        return None, {}
    
    suffix_weights = model[prefix]
    if not suffix_weights:
        return None, {}
    
    # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ì˜ suffix
    predicted = max(suffix_weights.items(), key=lambda x: x[1])[0]
    
    # ê°€ì¤‘ì¹˜ë¥¼ ë¹„ìœ¨ë¡œ ë³€í™˜
    total = sum(suffix_weights.values())
    ratios = {suffix: (weight / total * 100) for suffix, weight in suffix_weights.items()}
    
    return predicted, ratios

def predict_safety_first(model, prefix, recent_history=None, consecutive_mismatches=0):
    """
    ì•ˆì „ ìš°ì„  ì ì‘í˜• ì˜ˆì¸¡ (ë…ë¦½ì  êµ¬í˜„)
    
    ì—°ì† ë¶ˆì¼ì¹˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•ˆì „ ëª¨ë“œë¥¼ ìë™ìœ¼ë¡œ í™œì„±í™”
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸ (build_safety_first_modelë¡œ êµ¬ì¶•)
        prefix: ì˜ˆì¸¡í•  prefix ë¬¸ìì—´
        recent_history: ìµœê·¼ ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ [(predicted, actual, is_match), ...]
        consecutive_mismatches: í˜„ì¬ ì—°ì† ë¶ˆì¼ì¹˜ ìˆ˜
    
    Returns:
        dict: {
            'predicted': ì˜ˆì¸¡ê°’,
            'ratios': ë¹„ìœ¨ ë”•ì…”ë„ˆë¦¬,
            'confidence': ì‹ ë¢°ë„,
            'strategy_name': 'ì•ˆì „ìš°ì„ _ì ì‘í˜•',
            'is_safety_mode': ì•ˆì „ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€,
            'safety_reason': ì•ˆì „ ëª¨ë“œ í™œì„±í™” ì´ìœ 
        }
    """
    # 1. ê¸°ë³¸ ë¹ˆë„ ê¸°ë°˜ ì˜ˆì¸¡ (ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°)
    if prefix not in model:
        return {
            'predicted': None,
            'ratios': {},
            'confidence': 0.0,
            'strategy_name': 'ì•ˆì „ìš°ì„ _ì ì‘í˜•',
            'is_safety_mode': False,
            'safety_reason': None
        }
    
    suffix_counts = model[prefix]
    if not suffix_counts:
        return {
            'predicted': None,
            'ratios': {},
            'confidence': 0.0,
            'strategy_name': 'ì•ˆì „ìš°ì„ _ì ì‘í˜•',
            'is_safety_mode': False,
            'safety_reason': None
        }
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ suffix
    most_common = suffix_counts.most_common(1)[0]
    base_predicted = most_common[0]
    
    # ë¹„ìœ¨ ê³„ì‚°
    total = sum(suffix_counts.values())
    base_ratios = {suffix: (count / total * 100) for suffix, count in suffix_counts.items()}
    base_confidence = max(base_ratios.values()) if base_ratios else 0.0
    
    # 2. ì•ˆì „ ëª¨ë“œ íŒë‹¨
    is_safety_mode = False
    safety_reason = None
    predicted = base_predicted
    ratios = base_ratios.copy()
    
    # ì¡°ê±´ 1: ì—°ì† ë¶ˆì¼ì¹˜ê°€ 2íšŒ ì´ìƒì´ë©´ ì•ˆì „ ëª¨ë“œ
    if consecutive_mismatches >= 2:
        is_safety_mode = True
        safety_reason = f"ì—°ì† ë¶ˆì¼ì¹˜ {consecutive_mismatches}íšŒ"
        # ë°˜ëŒ€ ì˜ˆì¸¡ìœ¼ë¡œ ì „í™˜
        predicted = 'p' if base_predicted == 'b' else 'b'
        # ë¹„ìœ¨ë„ ë°˜ì „
        ratios = {'b': base_ratios.get('p', 0.0), 'p': base_ratios.get('b', 0.0)}
    
    # ì¡°ê±´ 2: ìµœê·¼ íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì„±ê³µë¥  ê³„ì‚°
    elif recent_history and len(recent_history) >= 5:
        recent = recent_history[-5:]  # ìµœê·¼ 5ê°œ
        recent_success_rate = sum(1 for h in recent if h[2]) / len(recent)  # is_matchê°€ Trueì¸ ë¹„ìœ¨
        
        # ìµœê·¼ ì„±ê³µë¥ ì´ 40% ë¯¸ë§Œì´ë©´ ì•ˆì „ ëª¨ë“œ
        if recent_success_rate < 0.4:
            is_safety_mode = True
            safety_reason = f"ìµœê·¼ ì„±ê³µë¥  {recent_success_rate*100:.1f}%"
            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ë°˜ëŒ€ ì˜ˆì¸¡
            if base_confidence < 60:
                predicted = 'p' if base_predicted == 'b' else 'b'
                ratios = {'b': base_ratios.get('p', 0.0), 'p': base_ratios.get('b', 0.0)}
    
    # ì¡°ê±´ 3: ì‹ ë¢°ë„ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ (45-55%) ì•ˆì „ ëª¨ë“œ
    elif 45 <= base_confidence <= 55:
        is_safety_mode = True
        safety_reason = f"ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìŒ ({base_confidence:.1f}%)"
        # ë°˜ëŒ€ ì˜ˆì¸¡ìœ¼ë¡œ ì „í™˜
        predicted = 'p' if base_predicted == 'b' else 'b'
        ratios = {'b': base_ratios.get('p', 0.0), 'p': base_ratios.get('b', 0.0)}
    
    # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
    confidence = max(ratios.values()) if ratios else 0.0
    
    return {
        'predicted': predicted,
        'ratios': ratios,
        'confidence': confidence,
        'strategy_name': 'ì•ˆì „ìš°ì„ _ì ì‘í˜•',
        'is_safety_mode': is_safety_mode,
        'safety_reason': safety_reason
    }

def predict_balance_recovery_trend_final(model, prefix):
    """
    ê· í˜• íšŒë³µ íŠ¸ë Œë“œ ëª¨ë¸ë¡œ ì˜ˆì¸¡ (í•­ìƒ ì˜ˆì¸¡ê°’ ë°˜í™˜ ë³´ì¥)
    
    Args:
        model: build_balance_recovery_trend_model_finalë¡œ êµ¬ì¶•ëœ ëª¨ë¸
        prefix: ì˜ˆì¸¡í•  prefix
    
    Returns:
        tuple: (predicted, ratios) - í•­ìƒ ê°’ ë°˜í™˜
    """
    # ê¸°ë³¸ ë¹ˆë„ ëª¨ë¸ ì˜ˆì¸¡ (í•­ìƒ ìˆìŒ)
    freq_predicted, freq_ratios = predict_frequency(model['frequency_model'], prefix)
    
    # prefix ê·œì¹™ì´ ìˆìœ¼ë©´ ì ìš©
    if prefix in model['prefix_rules']:
        rule = model['prefix_rules'][prefix]
        trend_follow = rule['trend_follow']
        confidence = rule['confidence']
        b_ratio = rule['b_ratio']
        
        # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡
        if trend_follow:
            # íŠ¸ë Œë“œ ë”°ë¦„: prefix ë¹„ìœ¨ê³¼ ê°™ì€ ë°©í–¥
            if b_ratio > 0.5:
                rule_predicted = 'b'
            elif b_ratio < 0.5:
                rule_predicted = 'p'
            else:
                # ê· í˜•ì´ë©´ ê¸°ë³¸ ë¹ˆë„ ëª¨ë¸ ì‚¬ìš©
                return freq_predicted, freq_ratios
        else:
            # íŠ¸ë Œë“œ ë°˜ëŒ€ (íšŒë³µ): prefix ë¹„ìœ¨ê³¼ ë°˜ëŒ€ ë°©í–¥
            if b_ratio > 0.5:
                rule_predicted = 'p'  # bê°€ ë§ìœ¼ë©´ p ì˜ˆì¸¡ (íšŒë³µ)
            elif b_ratio < 0.5:
                rule_predicted = 'b'  # pê°€ ë§ìœ¼ë©´ b ì˜ˆì¸¡ (íšŒë³µ)
            else:
                # ê· í˜•ì´ë©´ ê¸°ë³¸ ë¹ˆë„ ëª¨ë¸ ì‚¬ìš©
                return freq_predicted, freq_ratios
        
        # ê·œì¹™ê³¼ ë¹ˆë„ ëª¨ë¸ ê°€ì¤‘ í‰ê·  (ì‹ ë¢°ë„ ê¸°ë°˜)
        rule_weight = min(0.6, confidence * 1.2)  # ìµœëŒ€ 60%
        freq_weight = 1.0 - rule_weight
        
        # ê·œì¹™ ë¹„ìœ¨
        if rule_predicted == 'b':
            rule_b = 0.5 + (confidence * 0.25)
            rule_p = 1.0 - rule_b
        else:
            rule_p = 0.5 + (confidence * 0.25)
            rule_b = 1.0 - rule_p
        
        # ë¹ˆë„ ë¹„ìœ¨
        freq_b = freq_ratios.get('b', 50) / 100
        freq_p = freq_ratios.get('p', 50) / 100
        
        # ê°€ì¤‘ í‰ê· 
        combined_b = (rule_b * rule_weight) + (freq_b * freq_weight)
        combined_p = (rule_p * rule_weight) + (freq_p * freq_weight)
        
        # ì •ê·œí™”
        total = combined_b + combined_p
        if total > 0:
            combined_b = combined_b / total
            combined_p = combined_p / total
        
        predicted = 'b' if combined_b > combined_p else 'p'
        ratios = {
            'b': combined_b * 100,
            'p': combined_p * 100
        }
        
        return predicted, ratios
    
    # ê·œì¹™ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¹ˆë„ ëª¨ë¸ë§Œ ì‚¬ìš© (í•­ìƒ ì˜ˆì¸¡ê°’ ë°˜í™˜)
    return freq_predicted, freq_ratios

# ============================================================================
# íŒ¨í„´ ê²€ì¶œ í•¨ìˆ˜ë“¤ (ì‹ ê·œ)
# ============================================================================

def extract_prefix_suffix_sequence(window_size, prefix, min_occurrence=10):
    """
    íŠ¹ì • prefixì˜ suffix ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¶”ì¶œ
    
    Args:
        window_size: ìœˆë„ìš° í¬ê¸°
        prefix: ë¶„ì„í•  prefix
        min_occurrence: ìµœì†Œ ì¶œí˜„ íšŸìˆ˜
    
    Returns:
        dict: {
            'sequence': [suffix1, suffix2, ...],  # ì‹œê°„ ìˆœì„œ
            'timestamps': [created_at1, ...],
            'grid_string_ids': [id1, id2, ...],
            'total_count': ì´ ì¶œí˜„ íšŸìˆ˜
        }
    """
    conn = get_db_connection()
    if conn is None:
        return None
    
    try:
        query = """
            SELECT 
                nc.suffix,
                nc.grid_string_id,
                nc.chunk_index,
                pgs.created_at as grid_created_at
            FROM ngram_chunks nc
            JOIN preprocessed_grid_strings pgs ON nc.grid_string_id = pgs.id
            WHERE nc.window_size = ? AND nc.prefix = ?
            ORDER BY pgs.created_at ASC, nc.chunk_index ASC
        """
        
        df = pd.read_sql_query(query, conn, params=[window_size, prefix])
        
        if len(df) < min_occurrence:
            return None
        
        return {
            'sequence': df['suffix'].tolist(),
            'timestamps': df['grid_created_at'].tolist(),
            'grid_string_ids': df['grid_string_id'].tolist(),
            'total_count': len(df)
        }
    
    except Exception as e:
        st.error(f"Suffix ì‹œí€€ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return None
    finally:
        conn.close()

def detect_suffix_patterns(sequence):
    """
    Suffix ì‹œí€€ìŠ¤ì—ì„œ íŒ¨í„´ ê²€ì¶œ
    
    Args:
        sequence: suffix ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['b', 'p', 'b', 'b', ...])
    
    Returns:
        dict: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    if len(sequence) < 5:
        return None
    
    # ìˆ«ìë¡œ ë³€í™˜ (b=0, p=1)
    numeric_seq = [0 if s == 'b' else 1 for s in sequence]
    
    results = {
        'total_length': len(sequence),
        'b_count': sequence.count('b'),
        'p_count': sequence.count('p'),
        'b_ratio': sequence.count('b') / len(sequence),
        'p_ratio': sequence.count('p') / len(sequence)
    }
    
    # 1. Runs Test (ëœë¤ì„± ê²€ì •)
    runs = 1
    for i in range(1, len(numeric_seq)):
        if numeric_seq[i] != numeric_seq[i-1]:
            runs += 1
    
    n1 = results['b_count']
    n2 = results['p_count']
    n = len(sequence)
    
    if n1 > 0 and n2 > 0:
        # Runs test í†µê³„ëŸ‰
        expected_runs = (2 * n1 * n2) / (n1 + n2) + 1
        variance_runs = (2 * n1 * n2 * (2 * n1 * n2 - n1 - n2)) / ((n1 + n2) ** 2 * (n1 + n2 - 1))
        
        if variance_runs > 0:
            z_score = (runs - expected_runs) / np.sqrt(variance_runs)
            p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
            
            results['runs_test'] = {
                'runs_count': runs,
                'expected_runs': expected_runs,
                'z_score': z_score,
                'p_value': p_value,
                'is_random': p_value > 0.05,
                'interpretation': 'ëœë¤' if p_value > 0.05 else 'íŒ¨í„´_ì¡´ì¬'
            }
        else:
            results['runs_test'] = None
    else:
        results['runs_test'] = None
    
    # 2. íŠ¸ë Œë“œ ë¶„ì„ (ì„ í˜• íšŒê·€)
    x = np.arange(len(numeric_seq))
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, numeric_seq)
    
    results['trend_analysis'] = {
        'slope': slope,
        'r_squared': r_value ** 2,
        'p_value': p_value,
        'has_trend': p_value < 0.05,
        'trend_direction': 'P_ì¦ê°€' if slope > 0 else 'B_ì¦ê°€' if slope < 0 else 'ì—†ìŒ'
    }
    
    # 3. ìê¸°ìƒê´€ ë¶„ì„ (ì£¼ê¸°ì„± ê²€ì¶œ)
    max_lag = min(20, len(sequence) // 4)
    autocorrelations = []
    
    for lag in range(1, max_lag + 1):
        if len(sequence) > lag:
            seq1 = numeric_seq[:-lag]
            seq2 = numeric_seq[lag:]
            if len(seq1) > 0 and np.std(seq1) > 0 and np.std(seq2) > 0:
                corr = np.corrcoef(seq1, seq2)[0, 1]
                if not np.isnan(corr):
                    autocorrelations.append({'lag': lag, 'correlation': corr})
    
    if autocorrelations:
        max_corr = max(autocorrelations, key=lambda x: abs(x['correlation']))
        results['autocorrelation'] = {
            'max_correlation': max_corr['correlation'],
            'max_correlation_lag': max_corr['lag'],
            'has_periodicity': abs(max_corr['correlation']) > 0.3,
            'all_correlations': autocorrelations[:10]
        }
    else:
        results['autocorrelation'] = None
    
    # 4. ë§ˆë¥´ì½”í”„ ì²´ì¸ ë¶„ì„
    transitions = defaultdict(lambda: {'b': 0, 'p': 0})
    
    for i in range(len(sequence) - 1):
        current = sequence[i]
        next_suffix = sequence[i + 1]
        transitions[current][next_suffix] += 1
    
    markov_probs = {}
    for current, counts in transitions.items():
        total = counts['b'] + counts['p']
        if total > 0:
            markov_probs[current] = {
                'b_prob': counts['b'] / total,
                'p_prob': counts['p'] / total,
                'total': total
            }
    
    results['markov_chain'] = {
        'transition_probs': markov_probs,
        'has_dependency': len(markov_probs) > 0 and any(
            abs(prob['b_prob'] - prob['p_prob']) > 0.2 
            for prob in markov_probs.values()
        )
    }
    
    # 5. ìˆœí™˜ íŒ¨í„´ ê²€ì¶œ
    cycle_patterns = {}
    for cycle_len in [2, 3, 4, 5]:
        if len(sequence) >= cycle_len * 2:
            cycles = []
            for i in range(0, len(sequence) - cycle_len + 1, cycle_len):
                cycle = ''.join(sequence[i:i+cycle_len])
                cycles.append(cycle)
            
            if cycles:
                cycle_counter = Counter(cycles)
                most_common_cycle = cycle_counter.most_common(1)[0]
                cycle_ratio = most_common_cycle[1] / len(cycles)
                
                if cycle_ratio > 0.4:
                    cycle_patterns[cycle_len] = {
                        'pattern': most_common_cycle[0],
                        'frequency': most_common_cycle[1],
                        'ratio': cycle_ratio
                    }
    
    results['cycle_patterns'] = cycle_patterns if cycle_patterns else None
    
    # 6. ë³€í™”ì  ê²€ì¶œ
    window_size = max(5, len(sequence) // 10)
    change_points = []
    
    for i in range(window_size, len(sequence) - window_size):
        before = numeric_seq[i-window_size:i]
        after = numeric_seq[i:i+window_size]
        
        before_mean = np.mean(before)
        after_mean = np.mean(after)
        change_magnitude = abs(after_mean - before_mean)
        
        if change_magnitude > 0.3:
            change_points.append({
                'index': i,
                'change_magnitude': change_magnitude,
                'before_ratio': before_mean,
                'after_ratio': after_mean
            })
    
    results['change_points'] = change_points if change_points else None
    
    # 7. ì—°ì†ì„± ë¶„ì„
    max_consecutive_b = 0
    max_consecutive_p = 0
    current_b = 0
    current_p = 0
    
    for s in sequence:
        if s == 'b':
            current_b += 1
            current_p = 0
            max_consecutive_b = max(max_consecutive_b, current_b)
        else:
            current_p += 1
            current_b = 0
            max_consecutive_p = max(max_consecutive_p, current_p)
    
    results['consecutive_analysis'] = {
        'max_consecutive_b': max_consecutive_b,
        'max_consecutive_p': max_consecutive_p,
        'avg_consecutive_b': results['b_count'] / (sequence.count('bp') + sequence.count('pb') + 1),
        'avg_consecutive_p': results['p_count'] / (sequence.count('bp') + sequence.count('pb') + 1)
    }
    
    # 8. ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì¸¡ì •)
    def calculate_shannon_entropy(seq):
        """ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        if len(seq) == 0:
            return 0.0
        
        counts = Counter(seq)
        total = len(seq)
        
        entropy = 0.0
        for count in counts.values():
            if count > 0:
                probability = count / total
                entropy -= probability * log2(probability)
        
        # ì •ê·œí™” (ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ëŠ” log2(ê³ ìœ ê°’ ê°œìˆ˜))
        max_entropy = log2(len(counts)) if len(counts) > 0 else 1.0
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0
        
        return normalized_entropy
    
    entropy = calculate_shannon_entropy(sequence)
    results['shannon_entropy'] = {
        'entropy': entropy,
        'normalized_entropy': entropy,
        'predictability': 1.0 - entropy,  # ì˜ˆì¸¡ ê°€ëŠ¥ì„± (ë†’ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥)
        'interpretation': 'ì˜ˆì¸¡ê°€ëŠ¥' if entropy < 0.5 else 'ì¤‘ê°„' if entropy < 0.8 else 'ëœë¤'
    }
    
    # 9. ì´ë™ í‰ê·  ë° ë¹ˆë„ ë¶„ì„ (ë¹„ìœ¨ ë³€í™” ì§€ì  ê²€ì¶œ)
    def analyze_moving_average_frequency(seq, window_sz=100):
        """ì´ë™ í‰ê·  ë° ë¹ˆë„ ë¶„ì„"""
        if len(seq) < window_sz:
            window_sz = max(10, len(seq) // 2)
        
        numeric_seq = [0 if s == 'b' else 1 for s in seq]
        
        moving_ratios = []
        change_points = []
        
        for i in range(len(numeric_seq) - window_sz + 1):
            window = numeric_seq[i:i + window_sz]
            b_ratio = window.count(0) / len(window)
            p_ratio = window.count(1) / len(window)
            
            moving_ratios.append({
                'index': i + window_sz // 2,
                'b_ratio': b_ratio,
                'p_ratio': p_ratio,
                'imbalance': abs(b_ratio - p_ratio)
            })
        
        # ë³€í™”ì  ê²€ì¶œ
        if len(moving_ratios) > 1:
            for i in range(1, len(moving_ratios)):
                prev_ratio = moving_ratios[i-1]['b_ratio']
                curr_ratio = moving_ratios[i]['b_ratio']
                change_magnitude = abs(curr_ratio - prev_ratio)
                
                if change_magnitude > 0.2:
                    change_points.append({
                        'index': moving_ratios[i]['index'],
                        'change_magnitude': change_magnitude,
                        'prev_b_ratio': prev_ratio,
                        'curr_b_ratio': curr_ratio
                    })
        
        # ì „ì²´ í†µê³„
        if moving_ratios:
            avg_imbalance = np.mean([r['imbalance'] for r in moving_ratios])
            max_imbalance = max([r['imbalance'] for r in moving_ratios])
            avg_b_ratio = np.mean([r['b_ratio'] for r in moving_ratios])
            avg_p_ratio = np.mean([r['p_ratio'] for r in moving_ratios])
        else:
            avg_imbalance = 0.0
            max_imbalance = 0.0
            avg_b_ratio = 0.5
            avg_p_ratio = 0.5
        
        return {
            'window_size': window_sz,
            'avg_imbalance': avg_imbalance,
            'max_imbalance': max_imbalance,
            'avg_b_ratio': avg_b_ratio,
            'avg_p_ratio': avg_p_ratio,
            'change_points_count': len(change_points),
            'change_points': change_points[:10],
            'has_imbalance': avg_imbalance > 0.3,
            'interpretation': 'ë¶ˆê· í˜•_íŒ¨í„´' if avg_imbalance > 0.3 else 'ê· í˜•'
        }
    
    window_size_ma = min(100, len(sequence) // 2)
    if window_size_ma >= 10:
        moving_avg_result = analyze_moving_average_frequency(sequence, window_size_ma)
        results['moving_average'] = moving_avg_result
    else:
        results['moving_average'] = None
    
    # 10. íŒ¨í„´ ìš”ì•½
    pattern_summary = []
    
    if results['runs_test'] and not results['runs_test']['is_random']:
        pattern_summary.append("ë¹„ëœë¤_íŒ¨í„´")
    
    if results['trend_analysis']['has_trend']:
        pattern_summary.append(f"íŠ¸ë Œë“œ_{results['trend_analysis']['trend_direction']}")
    
    if results['autocorrelation'] and results['autocorrelation']['has_periodicity']:
        pattern_summary.append(f"ì£¼ê¸°ì„±_lag{results['autocorrelation']['max_correlation_lag']}")
    
    if results['markov_chain']['has_dependency']:
        pattern_summary.append("ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±")
    
    if results['cycle_patterns']:
        pattern_summary.append("ìˆœí™˜_íŒ¨í„´")
    
    if results['change_points']:
        pattern_summary.append(f"ë³€í™”ì _{len(results['change_points'])}ê°œ")
    
    # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì¶”ê°€
    if results.get('shannon_entropy'):
        entropy_info = results['shannon_entropy']
        if entropy_info['predictability'] > 0.5:
            pattern_summary.append("ì˜ˆì¸¡ê°€ëŠ¥")
        elif entropy_info['predictability'] < 0.2:
            pattern_summary.append("ëœë¤")
    
    # ì´ë™ í‰ê·  ê¸°ë°˜ íŒ¨í„´ ì¶”ê°€
    if results.get('moving_average') and results['moving_average'].get('has_imbalance'):
        pattern_summary.append("ë¶ˆê· í˜•_íŒ¨í„´")
    
    results['pattern_summary'] = pattern_summary if pattern_summary else ["ëœë¤_ë˜ëŠ”_ë³µí•©"]
    
    return results

def analyze_prefix_suffix_temporal_patterns(window_size, min_occurrence=10, top_n=50):
    """
    ëª¨ë“  prefixì— ëŒ€í•´ ì‹œê³„ì—´ suffix íŒ¨í„´ ë¶„ì„
    
    Args:
        window_size: ìœˆë„ìš° í¬ê¸°
        min_occurrence: ë¶„ì„í•  ìµœì†Œ ì¶œí˜„ íšŸìˆ˜
        top_n: ìƒìœ„ Nê°œ prefixë§Œ ìƒì„¸ ë¶„ì„
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    conn = get_db_connection()
    if conn is None:
        return None
    
    try:
        # prefixë³„ ì¶œí˜„ íšŸìˆ˜ ì§‘ê³„
        query = """
            SELECT 
                prefix,
                COUNT(*) as count
            FROM ngram_chunks
            WHERE window_size = ?
            GROUP BY prefix
            HAVING COUNT(*) >= ?
            ORDER BY COUNT(*) DESC
        """
        
        prefix_counts = pd.read_sql_query(query, conn, params=[window_size, min_occurrence])
        
        if len(prefix_counts) == 0:
            return None
        
        all_results = []
        detailed_results = {}
        
        # ìƒìœ„ Nê°œë§Œ ìƒì„¸ ë¶„ì„
        top_prefixes = prefix_counts.head(top_n)['prefix'].tolist()
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, row in prefix_counts.iterrows():
            prefix = row['prefix']
            count = row['count']
            
            if idx < len(top_prefixes):
                status_text.text(f"ìƒì„¸ ë¶„ì„ ì¤‘: {prefix} ({count}íšŒ)")
                progress_bar.progress((idx + 1) / min(len(prefix_counts), top_n))
            
            # ì‹œí€€ìŠ¤ ì¶”ì¶œ
            sequence_data = extract_prefix_suffix_sequence(window_size, prefix, min_occurrence)
            
            if sequence_data is None:
                continue
            
            # íŒ¨í„´ ê²€ì¶œ
            pattern_result = detect_suffix_patterns(sequence_data['sequence'])
            
            if pattern_result:
                result_entry = {
                    'prefix': prefix,
                    'total_count': count,
                    'b_ratio': pattern_result['b_ratio'],
                    'p_ratio': pattern_result['p_ratio'],
                    'pattern_summary': ', '.join(pattern_result['pattern_summary']),
                    'is_random': pattern_result.get('runs_test', {}).get('is_random', True) if pattern_result.get('runs_test') else True,
                    'has_trend': pattern_result.get('trend_analysis', {}).get('has_trend', False),
                    'has_periodicity': pattern_result.get('autocorrelation', {}).get('has_periodicity', False) if pattern_result.get('autocorrelation') else False,
                    'has_markov_dependency': pattern_result.get('markov_chain', {}).get('has_dependency', False),
                    'has_cycle': pattern_result.get('cycle_patterns') is not None,
                    'has_change_points': pattern_result.get('change_points') is not None
                }
                
                all_results.append(result_entry)
                
                # ìƒìœ„ Nê°œëŠ” ìƒì„¸ ê²°ê³¼ ì €ì¥
                if prefix in top_prefixes:
                    detailed_results[prefix] = {
                        'sequence_data': sequence_data,
                        'pattern_result': pattern_result
                    }
        
        progress_bar.empty()
        status_text.empty()
        
        return {
            'summary_df': pd.DataFrame(all_results),
            'detailed_results': detailed_results,
            'window_size': window_size,
            'min_occurrence': min_occurrence,
            'total_prefixes_analyzed': len(all_results)
        }
    
    except Exception as e:
        st.error(f"ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None
    finally:
        conn.close()

# ============================================================================
# ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ í•¨ìˆ˜ë“¤ (ì‹ ê·œ)
# ============================================================================

def build_all_models(ngrams_df, window_size, methods):
    """
    ëª¨ë“  ì˜ˆì¸¡ ë°©ë²•ì˜ ëª¨ë¸ì„ í•œë²ˆì— êµ¬ì¶•
    
    Args:
        ngrams_df: N-gram ì¡°ê° DataFrame
        window_size: ìœˆë„ìš° í¬ê¸°
        methods: ì‚¬ìš©í•  ì˜ˆì¸¡ ë°©ë²• ë¦¬ìŠ¤íŠ¸
    
    Returns:
        dict: {method_name: model}
    """
    models = {}
    
    if 'ë¹ˆë„ ê¸°ë°˜' in methods:
        models['ë¹ˆë„ ê¸°ë°˜'] = build_frequency_model(ngrams_df)
    
    if 'ê°€ì¤‘ì¹˜ ê¸°ë°˜' in methods:
        models['ê°€ì¤‘ì¹˜ ê¸°ë°˜'] = build_weighted_model(ngrams_df)
    
    if 'ì•ˆì „ ìš°ì„ ' in methods:
        models['ì•ˆì „ ìš°ì„ '] = build_safety_first_model(ngrams_df)
    
    if 'ê· í˜• íšŒë³µ íŠ¸ë Œë“œ' in methods:
        models['ê· í˜• íšŒë³µ íŠ¸ë Œë“œ'] = build_balance_recovery_trend_model_final(ngrams_df, window_size)
    
    return models

def compare_prediction_methods(models, prefixes, include_patterns=False, window_size=None):
    """
    ì—¬ëŸ¬ ì˜ˆì¸¡ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ DataFrame ë°˜í™˜
    
    Args:
        models: {method_name: model} ë”•ì…”ë„ˆë¦¬
        prefixes: ë¹„êµí•  prefix ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” DataFrame (prefix ì»¬ëŸ¼ í¬í•¨)
        include_patterns: íŒ¨í„´ ë¶„ì„ í¬í•¨ ì—¬ë¶€
        window_size: ìœˆë„ìš° í¬ê¸° (íŒ¨í„´ ë¶„ì„ ì‹œ í•„ìš”)
    
    Returns:
        pd.DataFrame: ë¹„êµ ê²°ê³¼
    """
    results = []
    
    # prefix ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    if isinstance(prefixes, pd.DataFrame):
        prefix_list = prefixes['prefix'].unique().tolist()
    else:
        prefix_list = prefixes
    
    for prefix in prefix_list:
        row = {'prefix': prefix}
        
        # ê° ë°©ë²•ë³„ ì˜ˆì¸¡
        for method_name, model in models.items():
            if method_name == 'ë¹ˆë„ ê¸°ë°˜':
                predicted, ratios = predict_frequency(model, prefix)
            elif method_name == 'ê°€ì¤‘ì¹˜ ê¸°ë°˜':
                predicted, ratios = predict_weighted(model, prefix)
            elif method_name == 'ì•ˆì „ ìš°ì„ ':
                result = predict_safety_first(model, prefix, recent_history=None, consecutive_mismatches=0)
                predicted = result.get('predicted')
                ratios = result.get('ratios', {})
            elif method_name == 'ê· í˜• íšŒë³µ íŠ¸ë Œë“œ':
                predicted, ratios = predict_balance_recovery_trend_final(model, prefix)
            else:
                predicted, ratios = None, {}
            
            confidence = max(ratios.values()) if ratios else 0.0
            
            row[f'{method_name}_ì˜ˆì¸¡'] = predicted
            row[f'{method_name}_ì‹ ë¢°ë„'] = confidence
            row[f'{method_name}_Bë¹„ìœ¨'] = ratios.get('b', 0.0)
            row[f'{method_name}_Pë¹„ìœ¨'] = ratios.get('p', 0.0)
        
        # íŒ¨í„´ ë¶„ì„ - 6ê°€ì§€ ë°©ë²•ë³„ë¡œ ìƒì„¸ ê²°ê³¼ í‘œì‹œ
        if include_patterns and window_size:
            sequence_data = extract_prefix_suffix_sequence(window_size, prefix, min_occurrence=5)
            if sequence_data:
                pattern_result = detect_suffix_patterns(sequence_data['sequence'])
                if pattern_result:
                    # 1. Runs Test ê²°ê³¼
                    runs_test = pattern_result.get('runs_test')
                    if runs_test:
                        row['RunsTest_ëœë¤ì—¬ë¶€'] = 'ëœë¤' if runs_test.get('is_random', True) else 'ë¹„ëœë¤'
                        row['RunsTest_pê°’'] = f"{runs_test.get('p_value', 0):.4f}"
                        row['RunsTest_runsìˆ˜'] = runs_test.get('runs_count', 0)
                    else:
                        row['RunsTest_ëœë¤ì—¬ë¶€'] = 'ë¶„ì„ë¶ˆê°€'
                        row['RunsTest_pê°’'] = '-'
                        row['RunsTest_runsìˆ˜'] = '-'
                    
                    # 2. íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
                    trend = pattern_result.get('trend_analysis', {})
                    row['íŠ¸ë Œë“œ_ë°©í–¥'] = trend.get('trend_direction', 'ì—†ìŒ')
                    row['íŠ¸ë Œë“œ_ìœ ì˜ì„±'] = 'ìœ ì˜í•¨' if trend.get('has_trend', False) else 'ìœ ì˜ì—†ìŒ'
                    row['íŠ¸ë Œë“œ_RÂ²'] = f"{trend.get('r_squared', 0):.4f}"
                    row['íŠ¸ë Œë“œ_pê°’'] = f"{trend.get('p_value', 1):.4f}"
                    
                    # 3. ìê¸°ìƒê´€ ë¶„ì„ (ì£¼ê¸°ì„±) ê²°ê³¼
                    autocorr = pattern_result.get('autocorrelation')
                    if autocorr:
                        row['ì£¼ê¸°ì„±_ì¡´ì¬'] = 'ìˆìŒ' if autocorr.get('has_periodicity', False) else 'ì—†ìŒ'
                        row['ì£¼ê¸°ì„±_ìµœëŒ€ìƒê´€'] = f"{autocorr.get('max_correlation', 0):.4f}"
                        row['ì£¼ê¸°ì„±_lag'] = autocorr.get('max_correlation_lag', '-')
                    else:
                        row['ì£¼ê¸°ì„±_ì¡´ì¬'] = 'ë¶„ì„ë¶ˆê°€'
                        row['ì£¼ê¸°ì„±_ìµœëŒ€ìƒê´€'] = '-'
                        row['ì£¼ê¸°ì„±_lag'] = '-'
                    
                    # 4. ë§ˆë¥´ì½”í”„ ì²´ì¸ ë¶„ì„ ê²°ê³¼
                    markov = pattern_result.get('markov_chain', {})
                    row['ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±'] = 'ìˆìŒ' if markov.get('has_dependency', False) else 'ì—†ìŒ'
                    transition_probs = markov.get('transition_probs', {})
                    if transition_probs:
                        if 'b' in transition_probs:
                            row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒBí™•ë¥ '] = f"{transition_probs['b'].get('b_prob', 0):.4f}"
                            row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒPí™•ë¥ '] = f"{transition_probs['b'].get('p_prob', 0):.4f}"
                        else:
                            row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒBí™•ë¥ '] = '-'
                            row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒPí™•ë¥ '] = '-'
                        if 'p' in transition_probs:
                            row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒBí™•ë¥ '] = f"{transition_probs['p'].get('b_prob', 0):.4f}"
                            row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒPí™•ë¥ '] = f"{transition_probs['p'].get('p_prob', 0):.4f}"
                        else:
                            row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒBí™•ë¥ '] = '-'
                            row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒPí™•ë¥ '] = '-'
                    else:
                        row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒBí™•ë¥ '] = '-'
                        row['ë§ˆë¥´ì½”í”„_Bë‹¤ìŒPí™•ë¥ '] = '-'
                        row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒBí™•ë¥ '] = '-'
                        row['ë§ˆë¥´ì½”í”„_Pë‹¤ìŒPí™•ë¥ '] = '-'
                    
                    # 5. ìˆœí™˜ íŒ¨í„´ ê²°ê³¼
                    cycles = pattern_result.get('cycle_patterns')
                    if cycles:
                        cycle_info = []
                        for cycle_len, cycle_data in cycles.items():
                            cycle_info.append(f"ê¸¸ì´{cycle_len}:{cycle_data['pattern']}({cycle_data['ratio']:.2%})")
                        row['ìˆœí™˜íŒ¨í„´'] = ', '.join(cycle_info) if cycle_info else 'ì—†ìŒ'
                    else:
                        row['ìˆœí™˜íŒ¨í„´'] = 'ì—†ìŒ'
                    
                    # 6. ë³€í™”ì  ê²€ì¶œ ê²°ê³¼
                    change_points = pattern_result.get('change_points')
                    if change_points:
                        row['ë³€í™”ì _ê°œìˆ˜'] = len(change_points)
                        if len(change_points) > 0:
                            max_change = max(change_points, key=lambda x: x['change_magnitude'])
                            row['ë³€í™”ì _ìµœëŒ€ë³€í™”ëŸ‰'] = f"{max_change['change_magnitude']:.4f}"
                            row['ë³€í™”ì _ìœ„ì¹˜'] = f"{max_change['index']}"
                        else:
                            row['ë³€í™”ì _ìµœëŒ€ë³€í™”ëŸ‰'] = '-'
                            row['ë³€í™”ì _ìœ„ì¹˜'] = '-'
                    else:
                        row['ë³€í™”ì _ê°œìˆ˜'] = 0
                        row['ë³€í™”ì _ìµœëŒ€ë³€í™”ëŸ‰'] = '-'
                        row['ë³€í™”ì _ìœ„ì¹˜'] = '-'
                    
                    # 7. ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼ ê²°ê³¼
                    entropy_info = pattern_result.get('shannon_entropy')
                    if entropy_info:
                        row['ì—”íŠ¸ë¡œí”¼_ê°’'] = f"{entropy_info.get('entropy', 0):.4f}"
                        row['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] = f"{entropy_info.get('predictability', 0):.4f}"
                        row['ì—”íŠ¸ë¡œí”¼_í•´ì„'] = entropy_info.get('interpretation', 'ì¤‘ê°„')
                    else:
                        row['ì—”íŠ¸ë¡œí”¼_ê°’'] = '-'
                        row['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] = '-'
                        row['ì—”íŠ¸ë¡œí”¼_í•´ì„'] = '-'
                    
                    # 8. ì´ë™ í‰ê·  ë° ë¹ˆë„ ë¶„ì„ ê²°ê³¼
                    moving_avg = pattern_result.get('moving_average')
                    if moving_avg:
                        row['ì´ë™í‰ê· _ë¶ˆê· í˜•'] = f"{moving_avg.get('avg_imbalance', 0):.4f}"
                        row['ì´ë™í‰ê· _ìµœëŒ€ë¶ˆê· í˜•'] = f"{moving_avg.get('max_imbalance', 0):.4f}"
                        row['ì´ë™í‰ê· _í‰ê· Bë¹„ìœ¨'] = f"{moving_avg.get('avg_b_ratio', 0):.4f}"
                        row['ì´ë™í‰ê· _í‰ê· Pë¹„ìœ¨'] = f"{moving_avg.get('avg_p_ratio', 0):.4f}"
                        row['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] = moving_avg.get('change_points_count', 0)
                        row['ì´ë™í‰ê· _í•´ì„'] = moving_avg.get('interpretation', 'ê· í˜•')
                    else:
                        row['ì´ë™í‰ê· _ë¶ˆê· í˜•'] = '-'
                        row['ì´ë™í‰ê· _ìµœëŒ€ë¶ˆê· í˜•'] = '-'
                        row['ì´ë™í‰ê· _í‰ê· Bë¹„ìœ¨'] = '-'
                        row['ì´ë™í‰ê· _í‰ê· Pë¹„ìœ¨'] = '-'
                        row['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] = '-'
                        row['ì´ë™í‰ê· _í•´ì„'] = '-'
                else:
                    # íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨
                    row['RunsTest_ëœë¤ì—¬ë¶€'] = 'ë¶„ì„ë¶ˆê°€'
                    row['íŠ¸ë Œë“œ_ë°©í–¥'] = 'ë¶„ì„ë¶ˆê°€'
                    row['ì£¼ê¸°ì„±_ì¡´ì¬'] = 'ë¶„ì„ë¶ˆê°€'
                    row['ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±'] = 'ë¶„ì„ë¶ˆê°€'
                    row['ìˆœí™˜íŒ¨í„´'] = 'ë¶„ì„ë¶ˆê°€'
                    row['ë³€í™”ì _ê°œìˆ˜'] = '-'
                    row['ì—”íŠ¸ë¡œí”¼_ê°’'] = '-'
                    row['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] = '-'
                    row['ì—”íŠ¸ë¡œí”¼_í•´ì„'] = '-'
                    row['ì´ë™í‰ê· _ë¶ˆê· í˜•'] = '-'
                    row['ì´ë™í‰ê· _ìµœëŒ€ë¶ˆê· í˜•'] = '-'
                    row['ì´ë™í‰ê· _í‰ê· Bë¹„ìœ¨'] = '-'
                    row['ì´ë™í‰ê· _í‰ê· Pë¹„ìœ¨'] = '-'
                    row['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] = '-'
                    row['ì´ë™í‰ê· _í•´ì„'] = '-'
            else:
                # ë°ì´í„° ë¶€ì¡±
                row['RunsTest_ëœë¤ì—¬ë¶€'] = 'ë°ì´í„°ë¶€ì¡±'
                row['íŠ¸ë Œë“œ_ë°©í–¥'] = 'ë°ì´í„°ë¶€ì¡±'
                row['ì£¼ê¸°ì„±_ì¡´ì¬'] = 'ë°ì´í„°ë¶€ì¡±'
                row['ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±'] = 'ë°ì´í„°ë¶€ì¡±'
                row['ìˆœí™˜íŒ¨í„´'] = 'ë°ì´í„°ë¶€ì¡±'
                row['ë³€í™”ì _ê°œìˆ˜'] = '-'
                row['ì—”íŠ¸ë¡œí”¼_ê°’'] = '-'
                row['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] = '-'
                row['ì—”íŠ¸ë¡œí”¼_í•´ì„'] = '-'
                row['ì´ë™í‰ê· _ë¶ˆê· í˜•'] = '-'
                row['ì´ë™í‰ê· _ìµœëŒ€ë¶ˆê· í˜•'] = '-'
                row['ì´ë™í‰ê· _í‰ê· Bë¹„ìœ¨'] = '-'
                row['ì´ë™í‰ê· _í‰ê· Pë¹„ìœ¨'] = '-'
                row['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] = '-'
                row['ì´ë™í‰ê· _í•´ì„'] = '-'
        
        results.append(row)
    
    return pd.DataFrame(results)

# ============================================================================
# ì‹œê³„ì—´ ëˆ„ì  ê²€ì¦ í•¨ìˆ˜ë“¤ (ì‹ ê·œ)
# ============================================================================

def simulate_step_by_step(model, grid_string, window_size, method="ë¹ˆë„ ê¸°ë°˜"):
    """
    ë‹¨ê³„ë³„ ì‹œë®¬ë ˆì´ì…˜ ë° ê²°ê³¼ ìˆ˜ì§‘
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        grid_string: ê²€ì¦í•  ë¬¸ìì—´
        window_size: ìœˆë„ìš° í¬ê¸°
        method: ì˜ˆì¸¡ ë°©ë²•
    
    Returns:
        list: ê° ìŠ¤í…ì˜ ê²°ê³¼
    """
    prefixes_data = extract_prefixes_from_string(grid_string, window_size)
    
    if not prefixes_data:
        return []
    
    history = []
    
    for step, (prefix, actual_suffix, index) in enumerate(prefixes_data):
        # ì˜ˆì¸¡ ìˆ˜í–‰
        if method == "ë¹ˆë„ ê¸°ë°˜":
            predicted, ratios = predict_frequency(model, prefix)
        elif method == "ê°€ì¤‘ì¹˜ ê¸°ë°˜":
            predicted, ratios = predict_weighted(model, prefix)
        elif method == "ì•ˆì „ ìš°ì„ ":
            result = predict_safety_first(model, prefix, recent_history=None, consecutive_mismatches=0)
            predicted = result.get('predicted')
            ratios = result.get('ratios', {})
        elif method == "ê· í˜• íšŒë³µ íŠ¸ë Œë“œ":
            predicted, ratios = predict_balance_recovery_trend_final(model, prefix)
        else:
            predicted, ratios = None, {}
        
        confidence = max(ratios.values()) if ratios else 0.0
        is_correct = predicted == actual_suffix if predicted else False
        
        history.append({
            'step': step + 1,
            'index': index,
            'prefix': prefix,
            'predicted': predicted,
            'actual': actual_suffix,
            'is_correct': is_correct,
            'confidence': confidence,
            'b_ratio': ratios.get('b', 0.0),
            'p_ratio': ratios.get('p', 0.0)
        })
    
    return history

def validate_cumulative_timeseries(window_size, methods, cutoff_grid_string_id=None):
    """
    ì‹œê³„ì—´ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ìˆ˜í–‰
    
    Args:
        window_size: ìœˆë„ìš° í¬ê¸°
        methods: ì‚¬ìš©í•  ì˜ˆì¸¡ ë°©ë²• ë¦¬ìŠ¤íŠ¸
        cutoff_grid_string_id: í•™ìŠµ ë°ì´í„° ê¸°ì¤€ ID (Noneì´ë©´ ì „ì²´)
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
    """
    df_strings = load_preprocessed_data()
    
    if len(df_strings) == 0:
        return None
    
    # created_at ê¸°ì¤€ ì •ë ¬
    df_sorted = df_strings.sort_values('created_at').reset_index(drop=True)
    
    # cutoff ê¸°ì¤€ í•„í„°ë§
    if cutoff_grid_string_id:
        train_df = df_sorted[df_sorted['id'] <= cutoff_grid_string_id]
        test_df = df_sorted[df_sorted['id'] > cutoff_grid_string_id]
    else:
        train_df = df_sorted.iloc[:len(df_sorted)//2]  # ì „ë°˜ë¶€
        test_df = df_sorted.iloc[len(df_sorted)//2:]    # í›„ë°˜ë¶€
    
    results_by_method = {}
    
    for method in methods:
        method_results = {
            'total_tested': 0,
            'total_steps': 0,
            'total_correct': 0,
            'total_incorrect': 0,
            'grid_string_results': []
        }
        
        # ê° í…ŒìŠ¤íŠ¸ grid_stringì— ëŒ€í•´
        for idx, row in test_df.iterrows():
            current_grid_string = row['grid_string']
            current_id = row['id']
            
            if len(current_grid_string) < window_size:
                continue
            
            # ì´ì „ê¹Œì§€ì˜ ëª¨ë“  grid_string ID (í˜„ì¬ ì œì™¸)
            previous_ids = train_df[train_df['id'] < current_id]['id'].tolist()
            
            if len(previous_ids) == 0:
                continue
            
            try:
                # ì´ì „ê¹Œì§€ì˜ ëˆ„ì  ë°ì´í„°ë¡œ ëª¨ë¸ êµ¬ì¶•
                train_ngrams = load_ngram_chunks(window_size=window_size, grid_string_ids=previous_ids)
                
                if len(train_ngrams) == 0:
                    continue
                
                # ëª¨ë¸ êµ¬ì¶•
                if method == "ë¹ˆë„ ê¸°ë°˜":
                    model = build_frequency_model(train_ngrams)
                elif method == "ê°€ì¤‘ì¹˜ ê¸°ë°˜":
                    model = build_weighted_model(train_ngrams)
                elif method == "ì•ˆì „ ìš°ì„ ":
                    model = build_safety_first_model(train_ngrams)
                elif method == "ê· í˜• íšŒë³µ íŠ¸ë Œë“œ":
                    model = build_balance_recovery_trend_model_final(train_ngrams, window_size)
                else:
                    continue
                
                # ë‹¨ê³„ë³„ ì‹œë®¬ë ˆì´ì…˜
                history = simulate_step_by_step(model, current_grid_string, window_size, method)
                
                if len(history) > 0:
                    correct_count = sum(1 for h in history if h['is_correct'])
                    total_count = len(history)
                    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0.0
                    
                    method_results['total_tested'] += 1
                    method_results['total_steps'] += total_count
                    method_results['total_correct'] += correct_count
                    method_results['total_incorrect'] += (total_count - correct_count)
                    
                    method_results['grid_string_results'].append({
                        'grid_string_id': current_id,
                        'total_steps': total_count,
                        'correct': correct_count,
                        'incorrect': total_count - correct_count,
                        'accuracy': accuracy,
                        'history': history
                    })
            
            except Exception as e:
                st.warning(f"Grid String ID {current_id} ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì „ì²´ ì •í™•ë„ ê³„ì‚°
        if method_results['total_steps'] > 0:
            method_results['overall_accuracy'] = (method_results['total_correct'] / method_results['total_steps'] * 100)
        else:
            method_results['overall_accuracy'] = 0.0
        
        results_by_method[method] = method_results
    
    return {
        'window_size': window_size,
        'methods': methods,
        'cutoff_grid_string_id': cutoff_grid_string_id,
        'results_by_method': results_by_method
    }

# ============================================================================
# ì‹œê°í™” í•¨ìˆ˜ë“¤ (ì‹ ê·œ)
# ============================================================================

def display_prediction_comparison_table(comparison_df):
    """ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ í…Œì´ë¸” í‘œì‹œ"""
    if len(comparison_df) == 0:
        st.warning("ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.dataframe(comparison_df, use_container_width=True, hide_index=True)

def display_pattern_analysis_results(analysis_result, selected_prefix=None):
    """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    if analysis_result is None:
        st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    summary_df = analysis_result['summary_df']
    detailed_results = analysis_result['detailed_results']
    
    st.subheader(f"ğŸ“Š Prefixë³„ Suffix ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (Window Size: {analysis_result['window_size']})")
    st.caption(f"ì´ {analysis_result['total_prefixes_analyzed']}ê°œ prefix ë¶„ì„")
    
    # íŒ¨í„´ ìš”ì•½ í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        random_count = len(summary_df[summary_df['is_random'] == True])
        pattern_count = len(summary_df[summary_df['is_random'] == False])
        st.metric("ëœë¤ íŒ¨í„´", random_count)
        st.metric("ë¹„ëœë¤ íŒ¨í„´", pattern_count)
    
    with col2:
        trend_count = len(summary_df[summary_df['has_trend'] == True])
        st.metric("íŠ¸ë Œë“œ ì¡´ì¬", trend_count)
    
    with col3:
        periodicity_count = len(summary_df[summary_df['has_periodicity'] == True])
        st.metric("ì£¼ê¸°ì„± ì¡´ì¬", periodicity_count)
    
    with col4:
        markov_count = len(summary_df[summary_df['has_markov_dependency'] == True])
        st.metric("ë§ˆë¥´ì½”í”„ ì˜ì¡´ì„±", markov_count)
    
    # íŒ¨í„´ íƒ€ì…ë³„ ë¶„í¬
    if 'pattern_summary' in summary_df.columns:
        pattern_types = summary_df['pattern_summary'].value_counts().head(10)
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=pattern_types.index,
            y=pattern_types.values,
            marker_color='#3498db'
        ))
        fig.update_layout(
            title="íŒ¨í„´ íƒ€ì…ë³„ Prefix ê°œìˆ˜",
            xaxis_title="íŒ¨í„´ íƒ€ì…",
            yaxis_title="Prefix ê°œìˆ˜",
            height=400,
            xaxis={'tickangle': -45}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ìš”ì•½ í…Œì´ë¸”
    display_columns = ['prefix', 'total_count', 'b_ratio', 'p_ratio', 
                      'pattern_summary', 'is_random', 'has_trend', 
                      'has_periodicity', 'has_markov_dependency']
    
    st.dataframe(
        summary_df[display_columns].round(3),
        use_container_width=True,
        hide_index=True
    )

def display_cumulative_validation_results(validation_result):
    """ì‹œê³„ì—´ ëˆ„ì  ê²€ì¦ ê²°ê³¼ í‘œì‹œ"""
    if validation_result is None:
        st.warning("ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    results_by_method = validation_result['results_by_method']
    
    st.subheader(f"ğŸ“ˆ ì‹œê³„ì—´ ëˆ„ì  ê²€ì¦ ê²°ê³¼ (Window Size: {validation_result['window_size']})")
    
    # ë°©ë²•ë³„ ìš”ì•½
    summary_data = []
    for method, result in results_by_method.items():
        summary_data.append({
            'ë°©ë²•': method,
            'í…ŒìŠ¤íŠ¸_Gridìˆ˜': result['total_tested'],
            'ì „ì²´_ìŠ¤í…': result['total_steps'],
            'ì •í™•í•œ_ì˜ˆì¸¡': result['total_correct'],
            'í‹€ë¦°_ì˜ˆì¸¡': result['total_incorrect'],
            'ì „ì²´_ì •í™•ë„': f"{result['overall_accuracy']:.2f}%"
        })
    
    summary_df = pd.DataFrame(summary_data)
    st.dataframe(summary_df, use_container_width=True, hide_index=True)
    
    # ë°©ë²•ë³„ ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
    if len(summary_data) > 0:
        fig = go.Figure()
        methods = [d['ë°©ë²•'] for d in summary_data]
        accuracies = [float(d['ì „ì²´_ì •í™•ë„'].replace('%', '')) for d in summary_data]
        
        fig.add_trace(go.Bar(
            x=methods,
            y=accuracies,
            marker_color='#3498db',
            text=[f"{a:.1f}%" for a in accuracies],
            textposition='outside'
        ))
        fig.update_layout(
            title="ë°©ë²•ë³„ ì „ì²´ ì •í™•ë„ ë¹„êµ",
            xaxis_title="ì˜ˆì¸¡ ë°©ë²•",
            yaxis_title="ì •í™•ë„ (%)",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ìƒì„¸ ê²°ê³¼ (íƒ­ìœ¼ë¡œ í‘œì‹œ)
    if len(results_by_method) > 0:
        method_tabs = st.tabs(list(results_by_method.keys()))
        
        for tab_idx, (method, result) in enumerate(results_by_method.items()):
            with method_tabs[tab_idx]:
                st.write(f"**{method} ìƒì„¸ ê²°ê³¼**")
                
                if len(result['grid_string_results']) > 0:
                    # Grid Stringë³„ ê²°ê³¼ í…Œì´ë¸”
                    grid_results = []
                    for gr in result['grid_string_results']:
                        grid_results.append({
                            'Grid String ID': gr['grid_string_id'],
                            'ìŠ¤í… ìˆ˜': gr['total_steps'],
                            'ì •í™•í•œ ì˜ˆì¸¡': gr['correct'],
                            'í‹€ë¦° ì˜ˆì¸¡': gr['incorrect'],
                            'ì •í™•ë„': f"{gr['accuracy']:.2f}%"
                        })
                    
                    grid_df = pd.DataFrame(grid_results)
                    st.dataframe(grid_df, use_container_width=True, hide_index=True)
                    
                    # íˆìŠ¤í† ë¦¬ ì°¨íŠ¸ (ì²« ë²ˆì§¸ grid_string)
                    if len(result['grid_string_results']) > 0:
                        first_history = result['grid_string_results'][0]['history']
                        if len(first_history) > 0:
                            st.write("**ì²« ë²ˆì§¸ Grid Stringì˜ ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬**")
                            
                            steps = [h['step'] for h in first_history]
                            predicted_numeric = [0 if h['predicted'] == 'b' else 1 for h in first_history]
                            actual_numeric = [0 if h['actual'] == 'b' else 1 for h in first_history]
                            
                            fig = go.Figure()
                            fig.add_trace(go.Scatter(
                                x=steps,
                                y=predicted_numeric,
                                mode='lines+markers',
                                name='ì˜ˆì¸¡ê°’ (0=B, 1=P)',
                                line=dict(color='blue', width=2)
                            ))
                            fig.add_trace(go.Scatter(
                                x=steps,
                                y=actual_numeric,
                                mode='lines+markers',
                                name='ì‹¤ì œê°’ (0=B, 1=P)',
                                line=dict(color='red', width=2, dash='dash')
                            ))
                            fig.update_layout(
                                title=f"Grid String ID {result['grid_string_results'][0]['grid_string_id']} ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬",
                                xaxis_title="ìŠ¤í…",
                                yaxis_title="ê°’ (0=B, 1=P)",
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# Main í•¨ìˆ˜ (Streamlit UI)
# ============================================================================

def main():
    st.title("ğŸ“Š Prefix Suffix ì˜ˆì¸¡ ë¹„êµ ë° íŒ¨í„´ ê²€ì¶œ")
    st.markdown("Prefixë³„ë¡œ ì—¬ëŸ¬ ì˜ˆì¸¡ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , 6ê°€ì§€ íŒ¨í„´ ê²€ì¶œ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    st.markdown("---")
    
    # ë°ì´í„° ë¡œë“œ
    df_strings = load_preprocessed_data()
    
    if len(df_strings) == 0:
        st.warning("âš ï¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„¤ì • ì˜ì—­
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        window_size = st.selectbox(
            "ìœˆë„ìš° í¬ê¸°",
            options=[5, 6, 7, 8, 9],
            index=2  # 7ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ
        )
    
    with col2:
        methods = st.multiselect(
            "ì˜ˆì¸¡ ë°©ë²• ì„ íƒ",
            options=["ë¹ˆë„ ê¸°ë°˜", "ê°€ì¤‘ì¹˜ ê¸°ë°˜", "ì•ˆì „ ìš°ì„ ", "ê· í˜• íšŒë³µ íŠ¸ë Œë“œ"],
            default=["ë¹ˆë„ ê¸°ë°˜", "ê°€ì¤‘ì¹˜ ê¸°ë°˜"]
        )
    
    with col3:
        include_patterns = st.checkbox("íŒ¨í„´ ë¶„ì„ í¬í•¨ (6ê°€ì§€ ë°©ë²•)", value=True)
    
    with col4:
        cutoff_id = st.selectbox(
            "í•™ìŠµ ë°ì´í„° ê¸°ì¤€",
            options=[None] + sorted(df_strings['id'].tolist()),
            format_func=lambda x: f"ì „ì²´" if x is None else f"ID {x}",
            key="cutoff_id"
        )
    
    st.markdown("---")
    
    # Prefix í•„í„°ë§ ì˜µì…˜
    col1, col2 = st.columns(2)
    with col1:
        min_occurrence = st.number_input("ìµœì†Œ ì¶œí˜„ íšŸìˆ˜", min_value=1, value=5, key="min_occ")
    with col2:
        prefix_search = st.text_input("Prefix ê²€ìƒ‰ (ì„ íƒì‚¬í•­)", key="prefix_search")
    
    if st.button("ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True):
        if len(methods) == 0:
            st.warning("ì˜ˆì¸¡ ë°©ë²•ì„ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë°ì´í„° ë¡œë”© ë° ëª¨ë¸ êµ¬ì¶• ì¤‘..."):
                # í•™ìŠµ ë°ì´í„° ì„¤ì •
                if cutoff_id:
                    train_ids = df_strings[df_strings['id'] <= cutoff_id]['id'].tolist()
                else:
                    train_ids = df_strings['id'].tolist()
                
                # í•™ìŠµ ë°ì´í„° ë¡œë“œ
                train_ngrams = load_ngram_chunks(window_size=window_size, grid_string_ids=train_ids)
                
                if len(train_ngrams) == 0:
                    st.warning("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # ëª¨ë¸ êµ¬ì¶•
                    models = build_all_models(train_ngrams, window_size, methods)
                    
                    # Prefix ëª©ë¡ ì¶”ì¶œ (ìµœì†Œ ì¶œí˜„ íšŸìˆ˜ í•„í„°ë§)
                    prefix_counts = train_ngrams.groupby('prefix').size().reset_index(name='count')
                    prefix_counts = prefix_counts[prefix_counts['count'] >= min_occurrence]
                    
                    if prefix_search:
                        prefix_counts = prefix_counts[prefix_counts['prefix'].str.contains(prefix_search, case=False)]
                    
                    if len(prefix_counts) == 0:
                        st.warning("ì¡°ê±´ì— ë§ëŠ” prefixê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.success(f"âœ… {len(prefix_counts)}ê°œ prefix ë¶„ì„")
                        
                        # ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (íŒ¨í„´ ë¶„ì„ í•­ìƒ í¬í•¨)
                        comparison_df = compare_prediction_methods(
                            models,
                            prefix_counts['prefix'].tolist(),
                            include_patterns=True,  # í•­ìƒ íŒ¨í„´ ë¶„ì„ í¬í•¨
                            window_size=window_size
                        )
                        
                        # ê²°ê³¼ í‘œì‹œ
                        st.subheader("ğŸ“‹ Prefixë³„ ì˜ˆì¸¡ ê²°ê³¼ ë° íŒ¨í„´ ê²€ì¶œ ê²°ê³¼")
                        st.caption("ê° prefixì— ëŒ€í•´ ì˜ˆì¸¡ ë°©ë²•ë³„ ê²°ê³¼ì™€ 6ê°€ì§€ íŒ¨í„´ ê²€ì¶œ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                        display_prediction_comparison_table(comparison_df)
                        
                        # íŒ¨í„´ ê²€ì¶œ ë°©ë²•ë³„ ìš”ì•½ í†µê³„
                        if include_patterns and len(comparison_df) > 0:
                            st.subheader("ğŸ“Š íŒ¨í„´ ê²€ì¶œ ë°©ë²•ë³„ ìš”ì•½ í†µê³„")
                            
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                if 'RunsTest_ëœë¤ì—¬ë¶€' in comparison_df.columns:
                                    runs_random = len(comparison_df[comparison_df['RunsTest_ëœë¤ì—¬ë¶€'] == 'ëœë¤'])
                                    runs_non_random = len(comparison_df[comparison_df['RunsTest_ëœë¤ì—¬ë¶€'] == 'ë¹„ëœë¤'])
                                    st.metric("Runs Test - ëœë¤", runs_random)
                                    st.metric("Runs Test - ë¹„ëœë¤", runs_non_random)
                            
                            with col2:
                                if 'íŠ¸ë Œë“œ_ìœ ì˜ì„±' in comparison_df.columns:
                                    trend_significant = len(comparison_df[comparison_df['íŠ¸ë Œë“œ_ìœ ì˜ì„±'] == 'ìœ ì˜í•¨'])
                                    st.metric("íŠ¸ë Œë“œ ë¶„ì„ - ìœ ì˜í•¨", trend_significant)
                            
                            with col3:
                                if 'ì£¼ê¸°ì„±_ì¡´ì¬' in comparison_df.columns:
                                    periodicity_yes = len(comparison_df[comparison_df['ì£¼ê¸°ì„±_ì¡´ì¬'] == 'ìˆìŒ'])
                                    st.metric("ì£¼ê¸°ì„± - ìˆìŒ", periodicity_yes)
                            
                            col4, col5, col6 = st.columns(3)
                            
                            with col4:
                                if 'ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±' in comparison_df.columns:
                                    markov_yes = len(comparison_df[comparison_df['ë§ˆë¥´ì½”í”„_ì˜ì¡´ì„±'] == 'ìˆìŒ'])
                                    st.metric("ë§ˆë¥´ì½”í”„ ì˜ì¡´ì„± - ìˆìŒ", markov_yes)
                            
                            with col5:
                                if 'ìˆœí™˜íŒ¨í„´' in comparison_df.columns:
                                    cycle_yes = len(comparison_df[comparison_df['ìˆœí™˜íŒ¨í„´'] != 'ì—†ìŒ'])
                                    st.metric("ìˆœí™˜ íŒ¨í„´ - ìˆìŒ", cycle_yes)
                            
                            with col6:
                                if 'ë³€í™”ì _ê°œìˆ˜' in comparison_df.columns:
                                    change_points_df = comparison_df[comparison_df['ë³€í™”ì _ê°œìˆ˜'] != '-']
                                    if len(change_points_df) > 0:
                                        change_points_df['ë³€í™”ì _ê°œìˆ˜'] = pd.to_numeric(change_points_df['ë³€í™”ì _ê°œìˆ˜'], errors='coerce')
                                        change_points_yes = len(change_points_df[change_points_df['ë³€í™”ì _ê°œìˆ˜'] > 0])
                                        st.metric("ë³€í™”ì  - ìˆìŒ", change_points_yes)
                                    else:
                                        st.metric("ë³€í™”ì  - ìˆìŒ", 0)
                            
                            # ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì¸¡ì • ì§€í‘œ
                            st.markdown("---")
                            st.subheader("ğŸ“ˆ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì¸¡ì • ì§€í‘œ")
                            
                            col7, col8 = st.columns(2)
                            
                            with col7:
                                if 'ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±' in comparison_df.columns:
                                    entropy_df = comparison_df[comparison_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] != '-']
                                    if len(entropy_df) > 0:
                                        entropy_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] = pd.to_numeric(entropy_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'], errors='coerce')
                                        predictable_count = len(entropy_df[entropy_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] > 0.5])
                                        random_count = len(entropy_df[entropy_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'] < 0.2])
                                        st.metric("ì—”íŠ¸ë¡œí”¼ - ì˜ˆì¸¡ê°€ëŠ¥ (ì˜ˆì¸¡ê°€ëŠ¥ì„± > 0.5)", predictable_count)
                                        st.metric("ì—”íŠ¸ë¡œí”¼ - ëœë¤ (ì˜ˆì¸¡ê°€ëŠ¥ì„± < 0.2)", random_count)
                                        
                                        # í‰ê·  ì˜ˆì¸¡ê°€ëŠ¥ì„±
                                        avg_predictability = entropy_df['ì—”íŠ¸ë¡œí”¼_ì˜ˆì¸¡ê°€ëŠ¥ì„±'].mean()
                                        st.metric("í‰ê·  ì˜ˆì¸¡ê°€ëŠ¥ì„±", f"{avg_predictability:.3f}")
                            
                            with col8:
                                if 'ì´ë™í‰ê· _í•´ì„' in comparison_df.columns:
                                    imbalance_df = comparison_df[comparison_df['ì´ë™í‰ê· _í•´ì„'] == 'ë¶ˆê· í˜•_íŒ¨í„´']
                                    st.metric("ì´ë™í‰ê·  - ë¶ˆê· í˜• íŒ¨í„´", len(imbalance_df))
                                    
                                    # ë³€í™”ì ì´ ìˆëŠ” prefix
                                    change_df = comparison_df[comparison_df['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] != '-']
                                    if len(change_df) > 0:
                                        change_df['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] = pd.to_numeric(change_df['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'], errors='coerce')
                                        change_points_yes = len(change_df[change_df['ì´ë™í‰ê· _ë³€í™”ì ìˆ˜'] > 0])
                                        st.metric("ì´ë™í‰ê·  - ë³€í™”ì  ìˆìŒ", change_points_yes)
                                        
                                        # í‰ê·  ë¶ˆê· í˜•
                                        imbalance_df2 = comparison_df[comparison_df['ì´ë™í‰ê· _ë¶ˆê· í˜•'] != '-']
                                        if len(imbalance_df2) > 0:
                                            imbalance_df2['ì´ë™í‰ê· _ë¶ˆê· í˜•'] = pd.to_numeric(imbalance_df2['ì´ë™í‰ê· _ë¶ˆê· í˜•'], errors='coerce')
                                            avg_imbalance = imbalance_df2['ì´ë™í‰ê· _ë¶ˆê· í˜•'].mean()
                                            st.metric("í‰ê·  ë¶ˆê· í˜•", f"{avg_imbalance:.3f}")

if __name__ == "__main__":
    main()

