# 가중치 기반 예측 로직 분석

## 1. 함수 구조

### `build_weighted_model(ngrams_df, weight_decay=0.95)`
- **위치**: `hypothesis_validation_app.py` 829-859줄
- **기능**: 최근 조각에 더 높은 가중치를 부여하여 모델 구축
- **로직**:
  1. `grid_string_id`별로 그룹화
  2. 각 그룹 내에서 `chunk_index` 기준 정렬 (최근 조각이 뒤에 위치)
  3. 가중치 계산: `weight = weight_decay ** (max_index - idx - 1)`
     - 최근 조각일수록 높은 가중치 (weight_decay=0.95이면 최근 조각은 0.95^0 = 1.0, 이전 조각은 0.95^1, 0.95^2, ...)
  4. 각 prefix-suffix 쌍에 가중치를 누적

### `predict_weighted(model, prefix)`
- **위치**: `hypothesis_validation_app.py` 861-877줄
- **기능**: 가중치 기반 모델로 예측 수행
- **로직**:
  1. prefix가 모델에 없으면 None 반환
  2. 해당 prefix의 suffix별 가중치 중 최대값 선택
  3. 가중치를 비율(%)로 변환하여 반환

### `predict_for_prefix(model, prefix, method)`
- **위치**: `hypothesis_validation_app.py` 1348-1378줄
- **기능**: method에 따라 적절한 예측 함수 호출
- **가중치 기반 처리**: `method == "가중치 기반"`이면 `predict_weighted()` 호출

### `save_or_update_predictions_for_historical_data()`
- **위치**: `hypothesis_validation_app.py` 515-685줄
- **기능**: 이전 데이터로 예측값을 계산하여 stored_predictions 테이블에 저장
- **가중치 기반 처리**: 
  - 589-590줄: `method == "가중치 기반"`이면 `build_weighted_model()` 호출
  - 611줄: `predict_for_prefix(model, prefix, method)` 호출

## 2. 확인해야 할 사항

### 문제점 1: 가중치 기반 데이터가 stored_predictions에 저장되었는지?
- stored_predictions 테이블에서 `method = "가중치 기반"`인 레코드가 있는지 확인
- 예측값 생성 시 `methods` 파라미터에 "가중치 기반"이 포함되었는지 확인

### 문제점 2: chunk_index 순서 문제
- 현재 로직은 각 `grid_string_id`별로 독립적으로 가중치를 계산
- `grid_string_id`가 클수록 더 최근 데이터라고 가정한다면, 전체적인 최근성을 반영하지 못할 수 있음
- 예: grid_string_id=100의 chunk_index=0과 grid_string_id=200의 chunk_index=0 중 어느 것이 더 최근인가?

### 문제점 3: grid_string_id 기준 최근성 반영
- 전체 데이터에서 `grid_string_id`가 클수록 최근 데이터라고 가정한다면, 가중치 계산에 `grid_string_id`도 포함해야 함
- 현재는 각 `grid_string_id` 내에서만 최근성을 판단

## 3. 확인 방법

```sql
-- stored_predictions 테이블에서 가중치 기반 데이터 확인
SELECT 
    method,
    COUNT(*) as count,
    COUNT(DISTINCT prefix) as unique_prefixes
FROM stored_predictions
WHERE window_size IN (5, 6, 7, 8)
GROUP BY method;
```

만약 "가중치 기반" 데이터가 없다면, 예측값 생성 시 `methods` 파라미터에 "가중치 기반"을 포함하지 않았을 가능성이 높습니다.

## 4. 개선 제안

만약 `grid_string_id`가 클수록 최근 데이터라면, 가중치 계산에 이를 반영:

```python
def build_weighted_model_improved(ngrams_df, weight_decay=0.95, id_weight_decay=0.99):
    """
    grid_string_id도 고려한 가중치 기반 모델
    """
    model = defaultdict(lambda: defaultdict(float))
    
    # 전체 grid_string_id 범위 계산
    max_grid_string_id = ngrams_df['grid_string_id'].max()
    
    grouped = ngrams_df.groupby('grid_string_id')
    
    for grid_string_id, group_df in grouped:
        # grid_string_id 기반 가중치 (큰 id일수록 높은 가중치)
        id_weight = id_weight_decay ** (max_grid_string_id - grid_string_id)
        
        group_df = group_df.sort_values('chunk_index')
        max_index = len(group_df)
        
        for idx, (_, row) in enumerate(group_df.iterrows()):
            # chunk_index 기반 가중치
            chunk_weight = weight_decay ** (max_index - idx - 1)
            
            # 최종 가중치 = id_weight * chunk_weight
            weight = id_weight * chunk_weight
            
            prefix = row['prefix']
            suffix = row['suffix']
            model[prefix][suffix] += weight
    
    return dict(model)
```
